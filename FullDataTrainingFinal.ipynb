{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, r2_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "# Function to fetch image\n",
        "def fetch_image(image_url):\n",
        "    try:\n",
        "        response = requests.get(image_url, timeout=5)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        image = image.resize((224, 224))  # Resize image to 224x224\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {image_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, feature_extractor, transform=None):\n",
        "        self.df = df\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_url = self.df.iloc[idx]['image_link']\n",
        "        image = fetch_image(image_url)\n",
        "        if image is None:\n",
        "            image = Image.new('RGB', (224, 224))  # Create a blank image if fetch fails\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        pixel_values = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "        entity_value = self.df.iloc[idx]['entity_values']\n",
        "        entity_unit = self.df.iloc[idx]['entity_value_units']\n",
        "        return pixel_values.squeeze(0), entity_value, entity_unit\n",
        "\n",
        "\n",
        "# ViT model for feature extraction\n",
        "class ViTFeatureExtractorModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ViTFeatureExtractorModel, self).__init__()\n",
        "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.hidden_size = self.vit.config.hidden_size\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        outputs = self.vit(pixel_values=pixel_values)\n",
        "        return outputs.last_hidden_state[:, 0, :]  # CLS token for each image\n",
        "\n",
        "# Function to extract features from ViT model\n",
        "def extract_features(model, dataloader):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels_values = []\n",
        "    labels_units = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for pixel_values, values, units in tqdm(dataloader, desc=\"Extracting Features\"):\n",
        "            pixel_values = pixel_values.to(device)\n",
        "            feature_rep = model(pixel_values)\n",
        "            features.append(feature_rep.cpu().numpy())\n",
        "            labels_values.append(values.numpy())\n",
        "            labels_units.append(np.array(units))\n",
        "\n",
        "    return np.concatenate(features), np.concatenate(labels_values), np.concatenate(labels_units)\n",
        "\n",
        "# Loading the dataset and preprocessing\n",
        "df = pd.read_csv('/content/processed_train_unit.csv')\n",
        "\n",
        "# df['log_entity_values'] = np.log1p(df['entity_value_nos'])\n",
        "scaler = StandardScaler()\n",
        "df['entity_values'] = scaler.fit_transform(df[['entity_value_nos']])\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['units'] = label_encoder.fit_transform(df['entity_value_units'])\n",
        "\n",
        "entity_name = 'depth'\n",
        "df = df[df['entity_name'] == entity_name]\n",
        "\n",
        "# Remove train-validation split and use the full dataset\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "full_dataset = CustomDataset(df, feature_extractor)\n",
        "\n",
        "full_loader = DataLoader(full_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit_model = ViTFeatureExtractorModel().to(device)\n",
        "\n",
        "# Extract features from the ViT model using the full dataset\n",
        "full_features, full_values, full_units = extract_features(vit_model, full_loader)\n",
        "\n",
        "# Encoding units for classification\n",
        "full_units = label_encoder.fit_transform(full_units)\n"
      ],
      "metadata": {
        "id": "4LvifOYWkQo8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "a173f6c9-3134-4564-f7a3-2f4f28e98ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features:   6%|â–Œ         | 84/1355 [01:51<28:05,  1.33s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4c7b21f10901>\u001b[0m in \u001b[0;36m<cell line: 104>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# Extract features from the ViT model using the full dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mfull_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# Encoding units for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-4c7b21f10901>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Extracting Features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mfeature_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VAKTNSPgmNww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model for classification\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        # self.fc3 = nn.Linear(256,256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        # x = self.relu(self.fc3(x))\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Define the CNN model for regression\n",
        "class CNNRegressor(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(CNNRegressor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 512)\n",
        "        # self.fc4 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(512, 1)  # Single output for regression\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        # x = self.relu(self.fc4(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "QAFyGcW5YQn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ESIF7oApmRhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and prepare CNN models for classification and regression\n",
        "input_size = full_features.shape[1]  # Updated to use full_features\n",
        "\n",
        "num_classes = len(np.unique(full_units))\n",
        "cnn_classifier = CNNClassifier(input_size, num_classes).to(device)\n",
        "cnn_regressor = CNNRegressor(input_size).to(device)\n",
        "\n",
        "# Define loss functions and optimizers\n",
        "classification_criterion = nn.CrossEntropyLoss()\n",
        "regression_criterion = nn.MSELoss()\n",
        "classification_optimizer = optim.Adam(cnn_classifier.parameters(), lr=0.001)\n",
        "regression_optimizer = optim.Adam(cnn_regressor.parameters(), lr=0.001)\n",
        "\n",
        "# Custom dataset for CNN\n",
        "class CustomDatasetCNN(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Prepare data loaders for CNN\n",
        "full_dataset_class = CustomDatasetCNN(full_features, full_units)  # Updated to use full_features and full_units\n",
        "full_dataset_reg = CustomDatasetCNN(full_features, full_values)  # Updated to use full_features and full_values\n",
        "\n",
        "full_loader_class = DataLoader(full_dataset_class, batch_size=32, shuffle=True, num_workers=4)\n",
        "full_loader_reg = DataLoader(full_dataset_reg, batch_size=32, shuffle=True, num_workers=4)\n"
      ],
      "metadata": {
        "id": "vkRsp0D6lBM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Create a directory to save the best models\n",
        "model_save_dir = \"best_models\"\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, num_epochs=100, is_classification=True):\n",
        "    best_metric = float('-inf')  # Best metric is set to -inf to track highest score\n",
        "    best_model_path = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        all_labels = []\n",
        "        all_preds = []\n",
        "\n",
        "        for features, labels in train_loader:\n",
        "            features = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "            if is_classification:\n",
        "                labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
        "            else:\n",
        "                labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "\n",
        "            if is_classification:\n",
        "                loss = criterion(outputs, labels)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "            else:\n",
        "                loss = criterion(outputs.squeeze(), labels)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_preds.extend(outputs.squeeze().detach().cpu().numpy())  # Ensure detach is used for regression\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "        # Calculate metrics on the training data\n",
        "        if is_classification:\n",
        "            f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "            print(f\"Training F1 Score: {f1:.4f}\")\n",
        "            if f1 > best_metric:\n",
        "                best_metric = f1\n",
        "                best_model_path = os.path.join(model_save_dir, f\"best_classifier_model_{entity_name}.pth\")\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "                print(f\"Saved best classifier model with F1 score: {best_metric:.4f}\")\n",
        "        else:\n",
        "            # Remove squeeze in predictions for safety when single output\n",
        "            all_preds = np.array(all_preds).flatten()  # Make sure predictions are properly flattened\n",
        "\n",
        "            r2 = r2_score(all_labels, all_preds)\n",
        "            print(f\"Training RÂ² Score: {r2:.4f}\")\n",
        "            if r2 > best_metric:\n",
        "                best_metric = r2\n",
        "                best_model_path = os.path.join(model_save_dir, f\"best_regressor_model_{entity_name}.pth\")\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "                print(f\"Saved best regressor model with RÂ² score: {best_metric:.4f}\")\n",
        "\n",
        "# Train and evaluate CNN models using full dataset\n",
        "print(\"Training CNN for Regression on full dataset...\")\n",
        "train_model(cnn_regressor, regression_criterion, regression_optimizer, full_loader_reg, num_epochs=100, is_classification=False)\n",
        "\n",
        "print(\"Training CNN for Classification on full dataset...\")\n",
        "train_model(cnn_classifier, classification_criterion, classification_optimizer, full_loader_class, num_epochs=100, is_classification=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAUwsQdJha2j",
        "outputId": "9ed779af-a60f-4588-c0e6-b727fd15266f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CNN for Regression on full dataset...\n",
            "Epoch [1/100], Loss: 0.0026\n",
            "Training RÂ² Score: 0.9976\n",
            "Saved best regressor model with RÂ² score: 0.9976\n",
            "Epoch [2/100], Loss: 0.0034\n",
            "Training RÂ² Score: 0.9969\n",
            "Epoch [3/100], Loss: 0.0041\n",
            "Training RÂ² Score: 0.9963\n",
            "Epoch [4/100], Loss: 0.0053\n",
            "Training RÂ² Score: 0.9953\n",
            "Epoch [5/100], Loss: 0.0073\n",
            "Training RÂ² Score: 0.9934\n",
            "Epoch [6/100], Loss: 0.0099\n",
            "Training RÂ² Score: 0.9912\n",
            "Epoch [7/100], Loss: 0.0212\n",
            "Training RÂ² Score: 0.9811\n",
            "Epoch [8/100], Loss: 0.0282\n",
            "Training RÂ² Score: 0.9747\n",
            "Epoch [9/100], Loss: 0.0150\n",
            "Training RÂ² Score: 0.9865\n",
            "Epoch [10/100], Loss: 0.0079\n",
            "Training RÂ² Score: 0.9929\n",
            "Epoch [11/100], Loss: 0.0050\n",
            "Training RÂ² Score: 0.9955\n",
            "Epoch [12/100], Loss: 0.0034\n",
            "Training RÂ² Score: 0.9969\n",
            "Epoch [13/100], Loss: 0.0028\n",
            "Training RÂ² Score: 0.9974\n",
            "Epoch [14/100], Loss: 0.0028\n",
            "Training RÂ² Score: 0.9975\n",
            "Epoch [15/100], Loss: 0.0029\n",
            "Training RÂ² Score: 0.9974\n",
            "Epoch [16/100], Loss: 0.0033\n",
            "Training RÂ² Score: 0.9970\n",
            "Epoch [17/100], Loss: 0.0039\n",
            "Training RÂ² Score: 0.9965\n",
            "Epoch [18/100], Loss: 0.0060\n",
            "Training RÂ² Score: 0.9946\n",
            "Epoch [19/100], Loss: 0.0087\n",
            "Training RÂ² Score: 0.9922\n",
            "Epoch [20/100], Loss: 0.0220\n",
            "Training RÂ² Score: 0.9802\n",
            "Epoch [21/100], Loss: 0.0267\n",
            "Training RÂ² Score: 0.9760\n",
            "Epoch [22/100], Loss: 0.0168\n",
            "Training RÂ² Score: 0.9850\n",
            "Epoch [23/100], Loss: 0.0100\n",
            "Training RÂ² Score: 0.9910\n",
            "Epoch [24/100], Loss: 0.0070\n",
            "Training RÂ² Score: 0.9938\n",
            "Epoch [25/100], Loss: 0.0063\n",
            "Training RÂ² Score: 0.9943\n",
            "Epoch [26/100], Loss: 0.0035\n",
            "Training RÂ² Score: 0.9968\n",
            "Epoch [27/100], Loss: 0.0031\n",
            "Training RÂ² Score: 0.9973\n",
            "Epoch [28/100], Loss: 0.0027\n",
            "Training RÂ² Score: 0.9975\n",
            "Epoch [29/100], Loss: 0.0028\n",
            "Training RÂ² Score: 0.9975\n",
            "Epoch [30/100], Loss: 0.0032\n",
            "Training RÂ² Score: 0.9971\n",
            "Epoch [31/100], Loss: 0.0038\n",
            "Training RÂ² Score: 0.9966\n",
            "Epoch [32/100], Loss: 0.0056\n",
            "Training RÂ² Score: 0.9950\n",
            "Epoch [33/100], Loss: 0.0063\n",
            "Training RÂ² Score: 0.9944\n",
            "Epoch [34/100], Loss: 0.0150\n",
            "Training RÂ² Score: 0.9866\n",
            "Epoch [35/100], Loss: 0.0151\n",
            "Training RÂ² Score: 0.9864\n",
            "Epoch [36/100], Loss: 0.0097\n",
            "Training RÂ² Score: 0.9912\n",
            "Epoch [37/100], Loss: 0.0060\n",
            "Training RÂ² Score: 0.9946\n",
            "Epoch [38/100], Loss: 0.0041\n",
            "Training RÂ² Score: 0.9963\n",
            "Epoch [39/100], Loss: 0.0034\n",
            "Training RÂ² Score: 0.9970\n",
            "Epoch [40/100], Loss: 0.0028\n",
            "Training RÂ² Score: 0.9975\n",
            "Epoch [41/100], Loss: 0.0032\n",
            "Training RÂ² Score: 0.9972\n",
            "Epoch [42/100], Loss: 0.0035\n",
            "Training RÂ² Score: 0.9969\n",
            "Epoch [43/100], Loss: 0.0043\n",
            "Training RÂ² Score: 0.9962\n",
            "Epoch [44/100], Loss: 0.0059\n",
            "Training RÂ² Score: 0.9947\n",
            "Epoch [45/100], Loss: 0.0095\n",
            "Training RÂ² Score: 0.9915\n",
            "Epoch [46/100], Loss: 0.0128\n",
            "Training RÂ² Score: 0.9885\n",
            "Epoch [47/100], Loss: 0.0109\n",
            "Training RÂ² Score: 0.9902\n",
            "Epoch [48/100], Loss: 0.0078\n",
            "Training RÂ² Score: 0.9930\n",
            "Epoch [49/100], Loss: 0.0051\n",
            "Training RÂ² Score: 0.9954\n",
            "Epoch [50/100], Loss: 0.0046\n",
            "Training RÂ² Score: 0.9959\n",
            "Epoch [51/100], Loss: 0.0049\n",
            "Training RÂ² Score: 0.9956\n",
            "Epoch [52/100], Loss: 0.0045\n",
            "Training RÂ² Score: 0.9959\n",
            "Epoch [53/100], Loss: 0.0038\n",
            "Training RÂ² Score: 0.9966\n",
            "Epoch [54/100], Loss: 0.0051\n",
            "Training RÂ² Score: 0.9954\n",
            "Epoch [55/100], Loss: 0.0043\n",
            "Training RÂ² Score: 0.9961\n",
            "Epoch [56/100], Loss: 0.0035\n",
            "Training RÂ² Score: 0.9969\n",
            "Epoch [57/100], Loss: 0.0035\n",
            "Training RÂ² Score: 0.9969\n",
            "Epoch [58/100], Loss: 0.0039\n",
            "Training RÂ² Score: 0.9965\n",
            "Epoch [59/100], Loss: 0.0046\n",
            "Training RÂ² Score: 0.9959\n",
            "Epoch [60/100], Loss: 0.0066\n",
            "Training RÂ² Score: 0.9941\n",
            "Epoch [61/100], Loss: 0.0124\n",
            "Training RÂ² Score: 0.9888\n",
            "Epoch [62/100], Loss: 0.0099\n",
            "Training RÂ² Score: 0.9911\n",
            "Epoch [63/100], Loss: 0.0080\n",
            "Training RÂ² Score: 0.9928\n",
            "Epoch [64/100], Loss: 0.0059\n",
            "Training RÂ² Score: 0.9947\n",
            "Epoch [65/100], Loss: 0.0099\n",
            "Training RÂ² Score: 0.9912\n",
            "Epoch [66/100], Loss: 0.0053\n",
            "Training RÂ² Score: 0.9952\n",
            "Epoch [67/100], Loss: 0.0037\n",
            "Training RÂ² Score: 0.9966\n",
            "Epoch [68/100], Loss: 0.0026\n",
            "Training RÂ² Score: 0.9977\n",
            "Saved best regressor model with RÂ² score: 0.9977\n",
            "Epoch [69/100], Loss: 0.0023\n",
            "Training RÂ² Score: 0.9979\n",
            "Saved best regressor model with RÂ² score: 0.9979\n",
            "Epoch [70/100], Loss: 0.0024\n",
            "Training RÂ² Score: 0.9978\n",
            "Epoch [71/100], Loss: 0.0027\n",
            "Training RÂ² Score: 0.9975\n",
            "Epoch [72/100], Loss: 0.0031\n",
            "Training RÂ² Score: 0.9972\n",
            "Epoch [73/100], Loss: 0.0044\n",
            "Training RÂ² Score: 0.9960\n",
            "Epoch [74/100], Loss: 0.0050\n",
            "Training RÂ² Score: 0.9955\n",
            "Epoch [75/100], Loss: 0.0054\n",
            "Training RÂ² Score: 0.9951\n",
            "Epoch [76/100], Loss: 0.0079\n",
            "Training RÂ² Score: 0.9930\n",
            "Epoch [77/100], Loss: 0.0181\n",
            "Training RÂ² Score: 0.9837\n",
            "Epoch [78/100], Loss: 0.0138\n",
            "Training RÂ² Score: 0.9876\n",
            "Epoch [79/100], Loss: 0.0098\n",
            "Training RÂ² Score: 0.9912\n",
            "Epoch [80/100], Loss: 0.0053\n",
            "Training RÂ² Score: 0.9952\n",
            "Epoch [81/100], Loss: 0.0032\n",
            "Training RÂ² Score: 0.9971\n",
            "Epoch [82/100], Loss: 0.0024\n",
            "Training RÂ² Score: 0.9979\n",
            "Epoch [83/100], Loss: 0.0021\n",
            "Training RÂ² Score: 0.9981\n",
            "Saved best regressor model with RÂ² score: 0.9981\n",
            "Epoch [84/100], Loss: 0.0018\n",
            "Training RÂ² Score: 0.9984\n",
            "Saved best regressor model with RÂ² score: 0.9984\n",
            "Epoch [85/100], Loss: 0.0017\n",
            "Training RÂ² Score: 0.9985\n",
            "Saved best regressor model with RÂ² score: 0.9985\n",
            "Epoch [86/100], Loss: 0.0017\n",
            "Training RÂ² Score: 0.9985\n",
            "Epoch [87/100], Loss: 0.0020\n",
            "Training RÂ² Score: 0.9982\n",
            "Epoch [88/100], Loss: 0.0028\n",
            "Training RÂ² Score: 0.9974\n",
            "Epoch [89/100], Loss: 0.0039\n",
            "Training RÂ² Score: 0.9965\n",
            "Epoch [90/100], Loss: 0.0061\n",
            "Training RÂ² Score: 0.9945\n",
            "Epoch [91/100], Loss: 0.0054\n",
            "Training RÂ² Score: 0.9952\n",
            "Epoch [92/100], Loss: 0.0048\n",
            "Training RÂ² Score: 0.9957\n",
            "Epoch [93/100], Loss: 0.0276\n",
            "Training RÂ² Score: 0.9752\n",
            "Epoch [94/100], Loss: 0.0188\n",
            "Training RÂ² Score: 0.9831\n",
            "Epoch [95/100], Loss: 0.0095\n",
            "Training RÂ² Score: 0.9915\n",
            "Epoch [96/100], Loss: 0.0044\n",
            "Training RÂ² Score: 0.9960\n",
            "Epoch [97/100], Loss: 0.0024\n",
            "Training RÂ² Score: 0.9978\n",
            "Epoch [98/100], Loss: 0.0018\n",
            "Training RÂ² Score: 0.9984\n",
            "Epoch [99/100], Loss: 0.0017\n",
            "Training RÂ² Score: 0.9985\n",
            "Epoch [100/100], Loss: 0.0018\n",
            "Training RÂ² Score: 0.9984\n",
            "Training CNN for Classification on full dataset...\n",
            "Epoch [1/100], Loss: 0.0008\n",
            "Training F1 Score: 0.9997\n",
            "Saved best classifier model with F1 score: 0.9997\n",
            "Epoch [2/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Saved best classifier model with F1 score: 0.9998\n",
            "Epoch [3/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [4/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [5/100], Loss: 0.0007\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [6/100], Loss: 0.0010\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [7/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [8/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [9/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [10/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [11/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [12/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [13/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [14/100], Loss: 0.0008\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [15/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [16/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [17/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [18/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [19/100], Loss: 0.0179\n",
            "Training F1 Score: 0.9958\n",
            "Epoch [20/100], Loss: 0.1782\n",
            "Training F1 Score: 0.9448\n",
            "Epoch [21/100], Loss: 0.0502\n",
            "Training F1 Score: 0.9851\n",
            "Epoch [22/100], Loss: 0.0078\n",
            "Training F1 Score: 0.9984\n",
            "Epoch [23/100], Loss: 0.0016\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [24/100], Loss: 0.0009\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [25/100], Loss: 0.0007\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [26/100], Loss: 0.0007\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [27/100], Loss: 0.0013\n",
            "Training F1 Score: 0.9995\n",
            "Epoch [28/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9995\n",
            "Epoch [29/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [30/100], Loss: 0.0007\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [31/100], Loss: 0.0010\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [32/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [33/100], Loss: 0.0008\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [34/100], Loss: 0.0028\n",
            "Training F1 Score: 0.9995\n",
            "Epoch [35/100], Loss: 0.0012\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [36/100], Loss: 0.0010\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [37/100], Loss: 0.0008\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [38/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [39/100], Loss: 0.0008\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [40/100], Loss: 0.0008\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [41/100], Loss: 0.0007\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [42/100], Loss: 0.0007\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [43/100], Loss: 0.0008\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [44/100], Loss: 0.0033\n",
            "Training F1 Score: 0.9989\n",
            "Epoch [45/100], Loss: 0.1606\n",
            "Training F1 Score: 0.9532\n",
            "Epoch [46/100], Loss: 0.0551\n",
            "Training F1 Score: 0.9829\n",
            "Epoch [47/100], Loss: 0.0137\n",
            "Training F1 Score: 0.9955\n",
            "Epoch [48/100], Loss: 0.0030\n",
            "Training F1 Score: 0.9990\n",
            "Epoch [49/100], Loss: 0.0012\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [50/100], Loss: 0.0007\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [51/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [52/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [53/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [54/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [55/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [56/100], Loss: 0.0009\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [57/100], Loss: 0.0219\n",
            "Training F1 Score: 0.9937\n",
            "Epoch [58/100], Loss: 0.0601\n",
            "Training F1 Score: 0.9811\n",
            "Epoch [59/100], Loss: 0.0446\n",
            "Training F1 Score: 0.9882\n",
            "Epoch [60/100], Loss: 0.0072\n",
            "Training F1 Score: 0.9979\n",
            "Epoch [61/100], Loss: 0.0086\n",
            "Training F1 Score: 0.9992\n",
            "Epoch [62/100], Loss: 0.0024\n",
            "Training F1 Score: 0.9992\n",
            "Epoch [63/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [64/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [65/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [66/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [67/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [68/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [69/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [70/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [71/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [72/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [73/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [74/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [75/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [76/100], Loss: 0.0011\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [77/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [78/100], Loss: 0.0003\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [79/100], Loss: 0.0007\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [80/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [81/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [82/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [83/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [84/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [85/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [86/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [87/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [88/100], Loss: 0.0011\n",
            "Training F1 Score: 0.9997\n",
            "Epoch [89/100], Loss: 0.0008\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [90/100], Loss: 0.1680\n",
            "Training F1 Score: 0.9535\n",
            "Epoch [91/100], Loss: 0.0376\n",
            "Training F1 Score: 0.9898\n",
            "Epoch [92/100], Loss: 0.0099\n",
            "Training F1 Score: 0.9968\n",
            "Epoch [93/100], Loss: 0.0016\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [94/100], Loss: 0.0006\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [95/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [96/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [97/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [98/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [99/100], Loss: 0.0004\n",
            "Training F1 Score: 0.9998\n",
            "Epoch [100/100], Loss: 0.0005\n",
            "Training F1 Score: 0.9998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the conversion factor dictionary (as shown above)\n",
        "conversion_factors = {\n",
        "    'gram': 1, 'kilogram': 1000, 'milligram': 0.001, 'microgram': 1e-6, 'ton': 1e6, 'ounce': 28.3495, 'pound': 453.592,\n",
        "    'millilitre': 1, 'litre': 1000, 'cup': 240, 'fluid ounce': 29.5735, 'quart': 946.353, 'gallon': 3785.41,\n",
        "    'cubic foot': 28316.8, 'cubic inch': 16.3871, 'decilitre': 100, 'centilitre': 10, 'pint': 473.176,\n",
        "    'centimetre': 1, 'metre': 100, 'millimetre': 0.1, 'inch': 2.54, 'foot': 30.48,\n",
        "    'volt': 1, 'watt': 1, 'kilowatt': 1000\n",
        "}\n",
        "\n",
        "# These are the max occurring units for each entity_name\n",
        "standard_units = {\n",
        "    'depth': 'centimetre',\n",
        "    'height': 'centimetre',\n",
        "    'item_volume': 'millilitre',\n",
        "    'item_weight': 'gram',\n",
        "    'maximum_weight_recommendation': 'kilogram',\n",
        "    'voltage': 'volt',\n",
        "    'wattage': 'watt',\n",
        "    'width': 'centimetre'\n",
        "}\n",
        "\n",
        "def inverse_transform(predicted_value, predicted_unit, entity_name):\n",
        "    \"\"\"\n",
        "    Convert the standardized predicted_value back to the predicted_unit.\n",
        "\n",
        "    predicted_value: The value predicted by the regression head in the standard unit.\n",
        "    predicted_unit: The unit predicted by the classification head.\n",
        "    entity_name: The type of entity (e.g., 'item_weight', 'height', etc.).\n",
        "    \"\"\"\n",
        "    standard_unit = standard_units[entity_name]  # Lookup the standard unit for the entity (e.g., 'kilogram')\n",
        "\n",
        "    if predicted_unit in conversion_factors and standard_unit in conversion_factors:\n",
        "        # Calculate the inverse conversion factor\n",
        "        inverse_conversion_factor = conversion_factors[standard_unit] / conversion_factors[predicted_unit]\n",
        "        # Apply inverse transformation to convert from standard unit to predicted unit\n",
        "        return predicted_value * inverse_conversion_factor\n",
        "    else:\n",
        "        return predicted_value  # If no valid conversion, return the original value\n",
        "\n",
        "# Assuming df now contains the 'predicted_value' (standardized) and 'predicted_unit' columns\n",
        "# df['final_value'] = df.apply(lambda row: inverse_transform(row['predicted_value'],\n",
        "#                                                           row['predicted_unit'],\n",
        "#                                                           row['entity_name']), axis=1)"
      ],
      "metadata": {
        "id": "OyLQvTM7rHmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_image(image_url, vit_model, regression_model, classification_model, feature_extractor, scaler, label_encoder, device):\n",
        "    # Fetch the image\n",
        "    image = fetch_image(image_url)\n",
        "    if image is None:\n",
        "        print(f\"Image at {image_url} could not be fetched.\")\n",
        "        return None\n",
        "\n",
        "    # Convert the image into pixel values using the feature extractor\n",
        "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "    pixel_values = pixel_values.to(device)\n",
        "\n",
        "    # Extract features using the ViT model\n",
        "    vit_model.eval()\n",
        "    with torch.no_grad():\n",
        "        features = vit_model(pixel_values)\n",
        "        features = features.cpu().numpy().reshape(1, -1)  # Reshape to match model input\n",
        "\n",
        "    # Predict the entity value (regression)\n",
        "    cnn_regressor.eval()\n",
        "    with torch.no_grad():\n",
        "        features_tensor = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "        predicted_value = regression_model(features_tensor).cpu().numpy().reshape(-1, 1)\n",
        "\n",
        "    # Inverse transform to get the original scale of entity values\n",
        "    original_value = scaler.inverse_transform(predicted_value)\n",
        "    original_value = np.expm1(original_value)\n",
        "    print(f\"Predicted numerical value (Original Scale): {original_value[0][0]}\")\n",
        "\n",
        "    # Predict the entity unit (classification)\n",
        "    cnn_classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        predicted_unit = classification_model(features_tensor)\n",
        "        _, predicted_class = torch.max(predicted_unit, 1)\n",
        "        predicted_class = predicted_class.cpu().numpy()\n",
        "\n",
        "    # Convert the predicted class back to the original unit label\n",
        "    predicted_unit_label = label_encoder.inverse_transform(predicted_class)\n",
        "    print(f\"Predicted unit (Classification): {predicted_unit_label[0]}\")\n",
        "\n",
        "    return original_value[0][0], predicted_unit_label[0]\n",
        "\n",
        "# Example usage for prediction\n",
        "image_url = 'https://m.media-amazon.com/images/I/71gSRbyXmoL.jpg'\n",
        "predicted_value, predicted_unit = predict_single_image(\n",
        "    image_url, vit_model, cnn_regressor, cnn_classifier, feature_extractor, scaler, label_encoder, device\n",
        ")\n",
        "\n",
        "predicted_value = inverse_transform(predicted_value,predicted_unit,entity_name)\n",
        "\n",
        "print(f\"Predicted Value: {predicted_value}\")\n",
        "print(f\"Predicted Unit: {predicted_unit}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWYTlhjOdnNW",
        "outputId": "0f2dfb74-e5ff-48d6-f518-1989e4bfb724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted numerical value (Original Scale): 261.1028747558594\n",
            "Predicted unit (Classification): cup\n",
            "Predicted Value: 1.0879286448160808\n",
            "Predicted Unit: cup\n"
          ]
        }
      ]
    }
  ]
}